{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dafd317",
   "metadata": {},
   "source": [
    "# Chit-Chat: Making Sense of My Class Conversations  \n",
    "\n",
    "## Project Overview  \n",
    "This project is me turning my own classâ€™s **Google Meet chats** into a full-blown NLP playground.  \n",
    "We talk a lot; sometimes about data, sometimes about life, sometimes about memes that shouldnâ€™t exist.  \n",
    "I figured: *why not analyze it?*  \n",
    "\n",
    "The idea is simple: **take raw chat transcripts â†’ turn them into structured data â†’ apply NLP + analytics â†’ uncover patterns about how we talk, what we talk about, and when we talk the most.**  \n",
    "\n",
    "Think of it as: *quantifying chaos*.  \n",
    "\n",
    "## Data  \n",
    "- Source: Class standup session chat exports (Tuesday â†’ Thursday).  \n",
    "- Format: .sbv subtitle-like files â†’ parsed into a dataframe with:  \n",
    "  - timestamp`  \n",
    "  - name`  \n",
    "  - text`  \n",
    "  - day`  \n",
    "  - start_time / end_time  \n",
    "  - hours_in_class (so I can track talkativeness across the session)  \n",
    "\n",
    "## What Iâ€™m Doing  \n",
    "\n",
    "### 1. **Exploratory Stuff**  \n",
    "- Word frequencies & n-grams â†’ who says what, and how often.  \n",
    "- TF-IDF â†’ unique vocab per student or day.  \n",
    "- Engagement timelines â†’ when does the energy peak? Are we more talkative in hour 1 vs hour 2?  \n",
    "\n",
    "### 2. **Topic Modeling**  \n",
    "- Using LDA / BERTopic to see the hidden themes.  \n",
    "- Expect clusters like:  \n",
    "  - *â€œdataset troubleshootingâ€*  \n",
    "  - *â€œpresentation anxietyâ€*  \n",
    "  - *â€œinside jokes no one outside class will getâ€*  \n",
    "\n",
    "### 3. **Clustering & Embeddings**  \n",
    "- Sentence embeddings + KMeans to group similar chats.  \n",
    "- See which convos naturally cluster together (questions, banter, actual work ðŸ‘€).  \n",
    "\n",
    "### 4. **Sentiment & Emotion**  \n",
    "- Track the mood of the class: are we positive, stressed, neutral?  \n",
    "- Emotion tagging (joy, frustration, confusion) to see how feelings flow during the session.  \n",
    "\n",
    "### 5. **Speaker Analysis**  \n",
    "- Who talks the most?  \n",
    "- Who introduces new topics vs who mostly reacts?  \n",
    "- Basically: who drives the vibe of the class.  \n",
    "\n",
    "### 6. **Recommender Angle**  \n",
    "- Build a â€œstudy buddyâ€ recommender system based on similarity of chats.  \n",
    "- If you sound like me in class, the algo might just recommend we team up.  \n",
    "\n",
    "### 7. **Advanced Fun**  \n",
    "- **Dialogue Act Classification** â†’ label lines as *Question / Answer / Joke / Instruction / Off-topic*.  \n",
    "- **Keyword Extraction** â†’ daily highlights without re-reading everything.  \n",
    "- **Summarization** â†’ generate auto â€œstandup recaps.â€  \n",
    "- **Network Analysis** â†’ build a social graph of who responds to who (aka, our class dynamics in one picture).    \n",
    "\n",
    "## Tech Stack  \n",
    "- **Pandas** for wrangling  \n",
    "- **NLTK / SpaCy** for preprocessing  \n",
    "- **Scikit-learn** for TF-IDF, clustering, and classification  \n",
    "- **BERTopic / Gensim** for topic modeling  \n",
    "- **NetworkX** for social graph analysis  \n",
    "- **Matplotlib / Seaborn / Plotly** for making the chaos look pretty  \n",
    "\n",
    "## Expected Insights  \n",
    "- Are we more talkative at the start, midway, or near the end?  \n",
    "- What do we actually talk about (vs what we *think* we talk about)?  \n",
    "- Who dominates conversations, and who stays quiet?  \n",
    "- Can we auto-summarize our standups without losing context?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d57f85",
   "metadata": {},
   "source": [
    "Import all relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9adcdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mathematical Operations\n",
    "import numpy as np\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "import seaborn as sns\n",
    "\n",
    "# String manipulation\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# NLP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Clustering\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# SVD\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Clustering metric scores\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Statistics & Scientific Computing\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "# Machine Learning - Model Selection & Evaluation\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Show entire column contents\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdac9642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stopwords from NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e6daf82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "# Check the english stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "# Check Swahili stopwords\n",
    "#print(stopwords.words('kiswahili'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfcba4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00:01:15.122,00:01:18.122\\n',\n",
       " 'William Arasirwa: lakini i need lessons on presentation\\n',\n",
       " '\\n',\n",
       " '00:10:39.341,00:10:42.341\\n',\n",
       " 'Stanley Njihia: Mnaskia reverb wakubwa\\n',\n",
       " '\\n',\n",
       " '00:10:47.810,00:10:50.810\\n',\n",
       " 'William Arasirwa: eeeeh\\n',\n",
       " '\\n',\n",
       " '00:14:14.814,00:14:17.814\\n',\n",
       " 'Stanley Njihia: Peris, cheza na massgrave io watermark iko apo bottom right iache kukusumbua.\\n',\n",
       " '\\n',\n",
       " '00:15:11.823,00:15:14.823\\n',\n",
       " 'William Arasirwa: address the elephant in the room\\n',\n",
       " '\\n',\n",
       " '00:15:43.275,00:15:46.275\\n',\n",
       " 'William Arasirwa: watu wa car dataset tuliona vumb\\n',\n",
       " '\\n',\n",
       " '00:16:02.430,00:16:05.430\\n',\n",
       " 'Stanley Njihia: ptsd\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Tuesday's chats\n",
    "tuesday_path = r\"C:\\Users\\lenovo\\OneDrive\\Desktop\\DS\\PROJECTS\\chit-chat\\Chatter box\\DSF-FT13R_P4_Standup - 2025_09_30 08_33 EAT - Chat 2.sbv\"\n",
    "\n",
    "with open(tuesday_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    tuesday_lines = f.readlines()\n",
    "\n",
    "tuesday_lines[:20]  # peek at structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c84fbc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00:00:06.928,00:00:09.928\\n',\n",
       " 'Ann-Felicity Mureithi: mnijibu\\n',\n",
       " '\\n',\n",
       " '00:00:10.928,00:00:13.928\\n',\n",
       " 'Norman Mwapea: Skia sentiments za gen alpha\\n',\n",
       " '\\n',\n",
       " '00:00:41.309,00:00:44.309\\n',\n",
       " 'Norman Mwapea: Kuhustle ka sjakosea. Ama ni aje Judith\\n',\n",
       " '\\n',\n",
       " '00:01:05.563,00:01:08.563\\n',\n",
       " 'Ann-Felicity Mureithi: @ stan wewe ebu confrim\\n',\n",
       " '\\n',\n",
       " '00:02:00.671,00:02:03.671\\n',\n",
       " 'Ann-Felicity Mureithi: aws\\n',\n",
       " '\\n',\n",
       " '00:02:22.962,00:02:25.962\\n',\n",
       " 'Norman Mwapea: Ati what tool did you use apart from gpt. That was just nasty\\n',\n",
       " '\\n',\n",
       " '00:10:30.379,00:10:33.379\\n',\n",
       " 'Ann-Felicity Mureithi: google search\\n']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Wednesday's chats\n",
    "wenno_path = r\"C:\\Users\\lenovo\\OneDrive\\Desktop\\DS\\PROJECTS\\chit-chat\\Chatter box\\DSF-FT13R_P4_Standup - 2025_10_01 08_31 EAT - Chat.sbv\"\n",
    "\n",
    "with open(wenno_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    wenno_lines = f.readlines()\n",
    "\n",
    "wenno_lines[:20]  # peek at structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "422ae4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00:00:00.075,00:00:03.075\\n',\n",
       " 'Norman Mwapea: Story inabamba sindio\\n',\n",
       " '\\n',\n",
       " '00:00:17.119,00:00:20.119\\n',\n",
       " 'Norman Mwapea: Mucene ya asubuhi. Morning gloru\\n',\n",
       " '\\n',\n",
       " '00:01:36.531,00:01:39.531\\n',\n",
       " 'Huldah Rotich: Kitts wa part time take a longer time so hatuwezi graduate na hao\\n',\n",
       " '\\n',\n",
       " '00:02:09.736,00:02:12.736\\n',\n",
       " 'Stanley Njihia: Vile ii mambo inaendelea itabidi Kitts ametolewa frontlines\\n',\n",
       " '\\n',\n",
       " '00:02:40.130,00:02:43.130\\n',\n",
       " 'William Arasirwa: ken ?\\n',\n",
       " '\\n',\n",
       " '00:02:43.832,00:02:46.832\\n',\n",
       " 'Jeff Kandie: Ken walibor\\n',\n",
       " '\\n',\n",
       " '00:02:48.896,00:02:51.896\\n',\n",
       " 'Jeff Kandie: a\\n']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Thursday's chats\n",
    "thur_path = r\"C:\\Users\\lenovo\\OneDrive\\Desktop\\DS\\PROJECTS\\chit-chat\\Chatter box\\DSF-FT13R_P4_Standup - 2025_10_02 08_32 EAT - Chat.sbv\"\n",
    "\n",
    "with open(thur_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    thur_lines = f.readlines()\n",
    "\n",
    "thur_lines[:20]  # peek at structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee52bf26",
   "metadata": {},
   "source": [
    "Good. The files are loaded and ready for manipulation. Since I want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dd3edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the corpus and convert them into dataframes\n",
    "def sbv_to_dataframe(file_path):\n",
    "    \n",
    "    '''Function to convert each file into a dataframe for easier analysis'''\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        content = f.read().strip()\n",
    "    \n",
    "    # Split by double newlines (each block = 1 subtitle)\n",
    "    blocks = content.split(\"\\n\\n\")\n",
    "    \n",
    "    data = []\n",
    "    for block in blocks:\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        if len(lines) >= 2:\n",
    "            timestamp = lines[0].strip()\n",
    "            line_text = lines[1].strip()\n",
    "            \n",
    "            # Split into name + text if \":\" exists\n",
    "            if \":\" in line_text:\n",
    "                name, text = line_text.split(\":\", 1)\n",
    "                name, text = name.strip(), text.strip()\n",
    "            else:\n",
    "                name, text = None, line_text  # fallback if no name\n",
    "            \n",
    "            data.append([timestamp, name, text])\n",
    "    \n",
    "    return pd.DataFrame(data, columns=[\"timestamp\", \"name\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8af191a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuesday:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>01:24:44.677,01:24:47.677</td>\n",
       "      <td>Jeff Mogaka</td>\n",
       "      <td>eishh</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>01:23:08.205,01:23:11.205</td>\n",
       "      <td>Jeff Mogaka</td>\n",
       "      <td>i give up</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>01:21:52.562,01:21:55.562</td>\n",
       "      <td>Jeff Mogaka</td>\n",
       "      <td>si wewe Fridah ulikua unalia ju ya JONTE phase 1</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>01:35:00.840,01:35:03.840</td>\n",
       "      <td>Jeff Mogaka</td>\n",
       "      <td>nitatokwa na uwazimu</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>01:15:49.048,01:15:52.048</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>cleanliness is key ya lef ama right?</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>01:43:03.178,01:43:06.178</td>\n",
       "      <td>Alvin kipleting</td>\n",
       "      <td>@ jeff ndio aura irudi naona itabidi umetumia OSINT on some people here</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>01:13:41.196,01:13:44.196</td>\n",
       "      <td>Stacy Mogeni</td>\n",
       "      <td>self love muhimu</td>\n",
       "      <td>2025-09-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp              name  \\\n",
       "137  01:24:44.677,01:24:47.677       Jeff Mogaka   \n",
       "123  01:23:08.205,01:23:11.205       Jeff Mogaka   \n",
       "114  01:21:52.562,01:21:55.562       Jeff Mogaka   \n",
       "172  01:35:00.840,01:35:03.840       Jeff Mogaka   \n",
       "78   01:15:49.048,01:15:52.048  William Arasirwa   \n",
       "215  01:43:03.178,01:43:06.178   Alvin kipleting   \n",
       "55   01:13:41.196,01:13:44.196      Stacy Mogeni   \n",
       "\n",
       "                                                                        text  \\\n",
       "137                                                                    eishh   \n",
       "123                                                                i give up   \n",
       "114                         si wewe Fridah ulikua unalia ju ya JONTE phase 1   \n",
       "172                                                     nitatokwa na uwazimu   \n",
       "78                                      cleanliness is key ya lef ama right?   \n",
       "215  @ jeff ndio aura irudi naona itabidi umetumia OSINT on some people here   \n",
       "55                                                          self love muhimu   \n",
       "\n",
       "          date  \n",
       "137 2025-09-30  \n",
       "123 2025-09-30  \n",
       "114 2025-09-30  \n",
       "172 2025-09-30  \n",
       "78  2025-09-30  \n",
       "215 2025-09-30  \n",
       "55  2025-09-30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wednesday:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>02:32:36.868,02:32:39.868</td>\n",
       "      <td>Alvin kipleting</td>\n",
       "      <td>get ready get ready</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>02:41:17.568,02:41:20.568</td>\n",
       "      <td>Jeff Mogaka</td>\n",
       "      <td>damn</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>01:54:09.703,01:54:12.703</td>\n",
       "      <td>Huldah Rotich</td>\n",
       "      <td>Hehehehe ati wananifukuza. Yho....I am right at home.</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>01:44:19.641,01:44:22.641</td>\n",
       "      <td>Alvin kipleting</td>\n",
       "      <td>watu wa Tz wanaumia</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00:11:22.853,00:11:25.853</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>chatgpt recieving shembeteng</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>01:50:00.110,01:50:03.110</td>\n",
       "      <td>Jeff Mogaka</td>\n",
       "      <td>*Judith</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>01:33:25.295,01:33:28.295</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>cha mkufuu mwana hu haa</td>\n",
       "      <td>2025-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp              name  \\\n",
       "225  02:32:36.868,02:32:39.868   Alvin kipleting   \n",
       "243  02:41:17.568,02:41:20.568       Jeff Mogaka   \n",
       "115  01:54:09.703,01:54:12.703     Huldah Rotich   \n",
       "76   01:44:19.641,01:44:22.641   Alvin kipleting   \n",
       "7    00:11:22.853,00:11:25.853  William Arasirwa   \n",
       "101  01:50:00.110,01:50:03.110       Jeff Mogaka   \n",
       "59   01:33:25.295,01:33:28.295  William Arasirwa   \n",
       "\n",
       "                                                      text       date  \n",
       "225                                    get ready get ready 2025-10-01  \n",
       "243                                                   damn 2025-10-01  \n",
       "115  Hehehehe ati wananifukuza. Yho....I am right at home. 2025-10-01  \n",
       "76                                     watu wa Tz wanaumia 2025-10-01  \n",
       "7                             chatgpt recieving shembeteng 2025-10-01  \n",
       "101                                                *Judith 2025-10-01  \n",
       "59                                 cha mkufuu mwana hu haa 2025-10-01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thursday:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>01:02:18.406,01:02:21.406</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>why cmd n</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>01:09:26.801,01:09:29.801</td>\n",
       "      <td>Kitts Kikumu</td>\n",
       "      <td>unajipiga own goal</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>00:29:08.550,00:29:11.550</td>\n",
       "      <td>Maureen Ngaire</td>\n",
       "      <td>Hulda siuseme ama nikuseme</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>01:22:22.812,01:22:25.812</td>\n",
       "      <td>Nesphory Mwadime</td>\n",
       "      <td>I might be wrong but I think the sample method generated different samples for everyone. Sikumbuki tukitumia random_state for reproducibilitty</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>01:45:16.237,01:45:19.237</td>\n",
       "      <td>Stanley Njihia</td>\n",
       "      <td>Pace ilichange haraka upesi</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>01:34:46.177,01:34:49.177</td>\n",
       "      <td>Alvin kipleting</td>\n",
       "      <td>vitu DS na MANU inanifanyia acha tu, Arasirwa hiyo shamba unabuy when</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>01:01:52.579,01:01:55.579</td>\n",
       "      <td>Ann-Felicity Mureithi</td>\n",
       "      <td>@norman and others</td>\n",
       "      <td>2025-10-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp                   name  \\\n",
       "107  01:02:18.406,01:02:21.406       William Arasirwa   \n",
       "114  01:09:26.801,01:09:29.801           Kitts Kikumu   \n",
       "26   00:29:08.550,00:29:11.550         Maureen Ngaire   \n",
       "122  01:22:22.812,01:22:25.812       Nesphory Mwadime   \n",
       "154  01:45:16.237,01:45:19.237         Stanley Njihia   \n",
       "146  01:34:46.177,01:34:49.177        Alvin kipleting   \n",
       "103  01:01:52.579,01:01:55.579  Ann-Felicity Mureithi   \n",
       "\n",
       "                                                                                                                                               text  \\\n",
       "107                                                                                                                                       why cmd n   \n",
       "114                                                                                                                              unajipiga own goal   \n",
       "26                                                                                                                       Hulda siuseme ama nikuseme   \n",
       "122  I might be wrong but I think the sample method generated different samples for everyone. Sikumbuki tukitumia random_state for reproducibilitty   \n",
       "154                                                                                                                     Pace ilichange haraka upesi   \n",
       "146                                                                           vitu DS na MANU inanifanyia acha tu, Arasirwa hiyo shamba unabuy when   \n",
       "103                                                                                                                              @norman and others   \n",
       "\n",
       "          date  \n",
       "107 2025-10-02  \n",
       "114 2025-10-02  \n",
       "26  2025-10-02  \n",
       "122 2025-10-02  \n",
       "154 2025-10-02  \n",
       "146 2025-10-02  \n",
       "103 2025-10-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the corpus as dataframes\n",
    "\n",
    "print(\"Tuesday:\")\n",
    "display(tuesday_df.sample(7))\n",
    "\n",
    "print(\"Wednesday:\")\n",
    "display(wenno_df.sample(7))\n",
    "\n",
    "print(\"Thursday:\")\n",
    "display(thur_df.sample(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce373ddf",
   "metadata": {},
   "source": [
    "Combine all the files into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba7427b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>day</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>hours_in_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:01:15.122,00:01:18.122</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>lakini i need lessons on presentation</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0 days 00:01:15.122000</td>\n",
       "      <td>0 days 00:01:18.122000</td>\n",
       "      <td>0.020867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:10:39.341,00:10:42.341</td>\n",
       "      <td>Stanley Njihia</td>\n",
       "      <td>Mnaskia reverb wakubwa</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0 days 00:10:39.341000</td>\n",
       "      <td>0 days 00:10:42.341000</td>\n",
       "      <td>0.177595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:10:47.810,00:10:50.810</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>eeeeh</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0 days 00:10:47.810000</td>\n",
       "      <td>0 days 00:10:50.810000</td>\n",
       "      <td>0.179947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:14:14.814,00:14:17.814</td>\n",
       "      <td>Stanley Njihia</td>\n",
       "      <td>Peris, cheza na massgrave io watermark iko apo bottom right iache kukusumbua.</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0 days 00:14:14.814000</td>\n",
       "      <td>0 days 00:14:17.814000</td>\n",
       "      <td>0.237448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:15:11.823,00:15:14.823</td>\n",
       "      <td>William Arasirwa</td>\n",
       "      <td>address the elephant in the room</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0 days 00:15:11.823000</td>\n",
       "      <td>0 days 00:15:14.823000</td>\n",
       "      <td>0.253284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>02:16:58.724,02:17:01.724</td>\n",
       "      <td>Alvin kipleting</td>\n",
       "      <td>inakata tu  naisha</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0 days 02:16:58.724000</td>\n",
       "      <td>0 days 02:17:01.724000</td>\n",
       "      <td>2.282979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>02:18:27.329,02:18:30.329</td>\n",
       "      <td>Kitts Kikumu</td>\n",
       "      <td>two months in two days</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0 days 02:18:27.329000</td>\n",
       "      <td>0 days 02:18:30.329000</td>\n",
       "      <td>2.307591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>02:18:30.920,02:18:33.920</td>\n",
       "      <td>Nesphory Mwadime</td>\n",
       "      <td>Enyewe Moringa ni business</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0 days 02:18:30.920000</td>\n",
       "      <td>0 days 02:18:33.920000</td>\n",
       "      <td>2.308589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>02:18:43.093,02:18:46.093</td>\n",
       "      <td>Stanley Njihia</td>\n",
       "      <td>eeiyy</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0 days 02:18:43.093000</td>\n",
       "      <td>0 days 02:18:46.093000</td>\n",
       "      <td>2.311970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>02:18:52.731,02:18:55.731</td>\n",
       "      <td>Norman Mwapea</td>\n",
       "      <td>Ann ako kwa jungle. Mara tunyau. Saii jogoo</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>0 days 02:18:52.731000</td>\n",
       "      <td>0 days 02:18:55.731000</td>\n",
       "      <td>2.314647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>746 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     timestamp              name  \\\n",
       "0    00:01:15.122,00:01:18.122  William Arasirwa   \n",
       "1    00:10:39.341,00:10:42.341    Stanley Njihia   \n",
       "2    00:10:47.810,00:10:50.810  William Arasirwa   \n",
       "3    00:14:14.814,00:14:17.814    Stanley Njihia   \n",
       "4    00:15:11.823,00:15:14.823  William Arasirwa   \n",
       "..                         ...               ...   \n",
       "741  02:16:58.724,02:17:01.724   Alvin kipleting   \n",
       "742  02:18:27.329,02:18:30.329      Kitts Kikumu   \n",
       "743  02:18:30.920,02:18:33.920  Nesphory Mwadime   \n",
       "744  02:18:43.093,02:18:46.093    Stanley Njihia   \n",
       "745  02:18:52.731,02:18:55.731     Norman Mwapea   \n",
       "\n",
       "                                                                              text  \\\n",
       "0                                            lakini i need lessons on presentation   \n",
       "1                                                           Mnaskia reverb wakubwa   \n",
       "2                                                                            eeeeh   \n",
       "3    Peris, cheza na massgrave io watermark iko apo bottom right iache kukusumbua.   \n",
       "4                                                 address the elephant in the room   \n",
       "..                                                                             ...   \n",
       "741                                                             inakata tu  naisha   \n",
       "742                                                         two months in two days   \n",
       "743                                                     Enyewe Moringa ni business   \n",
       "744                                                                          eeiyy   \n",
       "745                                    Ann ako kwa jungle. Mara tunyau. Saii jogoo   \n",
       "\n",
       "          day             start_time               end_time  hours_in_class  \n",
       "0     Tuesday 0 days 00:01:15.122000 0 days 00:01:18.122000        0.020867  \n",
       "1     Tuesday 0 days 00:10:39.341000 0 days 00:10:42.341000        0.177595  \n",
       "2     Tuesday 0 days 00:10:47.810000 0 days 00:10:50.810000        0.179947  \n",
       "3     Tuesday 0 days 00:14:14.814000 0 days 00:14:17.814000        0.237448  \n",
       "4     Tuesday 0 days 00:15:11.823000 0 days 00:15:14.823000        0.253284  \n",
       "..        ...                    ...                    ...             ...  \n",
       "741  Thursday 0 days 02:16:58.724000 0 days 02:17:01.724000        2.282979  \n",
       "742  Thursday 0 days 02:18:27.329000 0 days 02:18:30.329000        2.307591  \n",
       "743  Thursday 0 days 02:18:30.920000 0 days 02:18:33.920000        2.308589  \n",
       "744  Thursday 0 days 02:18:43.093000 0 days 02:18:46.093000        2.311970  \n",
       "745  Thursday 0 days 02:18:52.731000 0 days 02:18:55.731000        2.314647  \n",
       "\n",
       "[746 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 0. assign actual class dates (adjust if your dates differ) ---\n",
    "tuesday_df[\"date\"] = pd.to_datetime(\"2025-09-30\")\n",
    "wenno_df[\"date\"]   = pd.to_datetime(\"2025-10-01\")\n",
    "thur_df[\"date\"]    = pd.to_datetime(\"2025-10-02\")\n",
    "\n",
    "# --- 1. combine ---\n",
    "chat_df = pd.concat([tuesday_df, wenno_df, thur_df], ignore_index=True)\n",
    "\n",
    "# --- 2. extract start & end from timestamp ---\n",
    "def _extract_start_end(ts):\n",
    "\n",
    "    if pd.isna(ts):\n",
    "        return (None, None)\n",
    "\n",
    "    s = str(ts).strip()\n",
    "\n",
    "    if \",\" in s: \n",
    "        left, right = [p.strip() for p in s.split(\",\", 1)]\n",
    "        return (left or None, right or None)\n",
    "    matches = re.findall(r\"\\d{1,2}:\\d{2}:\\d{2}(?:\\.\\d+)?\", s)\n",
    "\n",
    "    if len(matches) >= 2:\n",
    "        return (matches[0], matches[1])\n",
    "\n",
    "    if len(matches) == 1:\n",
    "        return (matches[0], None)\n",
    "\n",
    "    return (None, None)\n",
    "\n",
    "times = chat_df[\"timestamp\"].apply(_extract_start_end)\n",
    "chat_df[[\"start_time_str\", \"end_time_str\"]] = pd.DataFrame(times.tolist(), index=chat_df.index)\n",
    "\n",
    "# --- 3. convert to Timedelta and compute hours in class ---\n",
    "chat_df[\"start_time\"] = pd.to_timedelta(chat_df[\"start_time_str\"], errors=\"coerce\")\n",
    "chat_df[\"end_time\"]   = pd.to_timedelta(chat_df[\"end_time_str\"], errors=\"coerce\")\n",
    "chat_df[\"hours_in_class\"] = chat_df[\"start_time\"].dt.total_seconds() / 3600.0\n",
    "\n",
    "# --- 4. add day column ---\n",
    "chat_df[\"day\"] = chat_df[\"date\"].dt.day_name()\n",
    "\n",
    "# --- 5. final dataframe ---\n",
    "final_df = chat_df[[\"timestamp\", \"name\", \"text\", \"day\", \"start_time\", \"end_time\", \"hours_in_class\"]].copy()\n",
    "\n",
    "# Preview\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eedc502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
